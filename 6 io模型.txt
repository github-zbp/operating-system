一、什么是io？
在计算机系统中I/O就是输入（Input）和输出(Output)的意思，针对不同的操作对象，可以划分为磁盘I/O模型，网络I/O模型，内存映射I/O, Direct I/O、数据库I/O等，只要具有输入输出类型的交互系统都可以认为是I/O系统

在Linux系统中，计算机里面的程序（代码程序）都是作为文件存在硬盘里面的。开机之后，这些程序就会从硬盘加载到内存并经过处理，变成CPU可执行的格式，此时这些程序就变成了进程。

kernel（内核）是开机是第一个加载到内存的程序。其他的程序都是有内核帮忙加载进内存的。内核进程和其他进程都会按照时间片占有CPU并发的运行（后面说到内核就是说内核程序内核进程）。

内核kernel：
内核会暴露一些系统调用system call（就是操作系统提供的函数），其他的进程可以调用内核的函数方便工作。
系统调用的作用就是，让其他进程调用内核提供的接口函数，让内核程序帮这些进程去完成这个操作，而不用其他进程自己去做这些事。如果是普通进程自己去做这些事，就需要他们自己重新实现这些方法，很麻烦，所以内核提供的这些系统调用大大减少了其他进程的工作量。

为了防止其他程序知道内核所在的内存地址修改内核kernel中的指令，Linux提供了一种保护模式，内核进程是处于这种保护模式之下（这种保护模式的作用就是不让其他程序知道内核所在的内存地址，这样其他进程就无法访问内核和修改内核）：当cpu执行内核进程中的指令时，这个指令可以访问其他进程的内存，但是cpu执行其他进程中的指令时，这个指令不能访问内核和其他进程的内存地址，只能访问自己这个进程的内存（也就是说，kernel可以访问和修改其他进程的内容，其他的进程只能访问和修改自己这个进程的内容）。

但是这个时候有点打脸，其他进程不能访问到内核又怎么调用到内核提供的系统调用（函数）呢？

这个时候就提出了系统中断的概念。


系统中断（很重要）
系统中断是计算机可以并发运行多个程序的关键。
所谓的中断其实就是告诉CPU停止运行当前的程序去做另一件事情，去做另一件事情有很多种情况

中断发生的情况有很多种
可能是时间片用完了，cpu接受到中断指令，于是cpu就停止当前程序的运行让下一个程序进程占用进行工作
也可能是程序中运行到sleep,yield这样的代码，运行到这样的代码也会发送中断指令给cpu，cpu就也会中断当前程序的运行，让其他进程运行
也可能是移动鼠标，鼠标会发送一个中断指令给cpu，cpu就会中断当前进程的运行，然后发送一个io请求让鼠标移动。

中断是一个计算机指令，这个指令后面会跟一个数字，这些数字映射到一个存着回调函数的表中（这个表叫做中断向量表，是存在CPU的寄存器中的，只有当发起中断指令给cpu时，cpu才会往这个表里面查数字对应的回调函数），一个数字代表一个回调函数。所以中断指令后面跟着的数字代表cpu中断程序后，会做些什么操作，是去调度下一个程序还是去发起一个io请求之类的。


反正一句话：中断就是告诉cpu停下手里的工作去干另一件事，干完这件事之后你可以继续运行刚刚的程序，也可以去运行其他程序。

如果没有中断，那么cpu就会跑完一个程序再跑另一个程序或者干其他事情。那计算机就变成串行运行程序而不是并发运行。


现在回到其他进程怎么调用内核的系统调用。比如，我用python调用了一个write函数，这个write函数里面其实埋了一个中断指令(int 0x80，int就是中断指令，这是cpu才能识别的指令)，当cpu运行到这个int 0x80的时候，就会找中断向量表对应0x80的回调函数并执行，回调函数会让cpu中断当前进程，并切换到内核进程（让内核进程占有cpu），内核再去调用系统调用中的write方法。
这个过程中，就由用户态的程序切换到了内核态，进入到了内核态就自然可以调内核中的函数了。不是用户进程去调内核的函数，而是切换到内核态后内核自己调自己进程的方法，是内核自己访问自己的内存。

io操作都会需要进行系统调用（调用内核提供的函数），所以io操作都需要系统中断（中断会让出cpu），而且得经过一个用户态内核态的切换。
所以io操作的成本比较高。

系统调用的成本比函数调用高（函数调用不需要切换态）


总结：系统调用需要进行系统中断，切换用户态和内核态，系统中断就要当前程序让出cpu停止工作，把cpu交给内核。
io操作都需要系统调用。（系统调用是一个专有名词）


二、从BIO到多路复用

1.BIO：同步阻塞I/O模式

以下面的代码为例：
先是服务端代码：
# coding=utf-8

from threading import Thread, currentThread
import socket

# 服务端代码

# 创建套接字
server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# 绑定ip和端口
ip = "127.0.0.1"
port = 8000
server.bind((ip, port))

# 监听套接字
server.listen()

print("服务已开启")

def contact(client):
    print("客户端 %s 已成功连接" % currentThread().name)

    msg = client.recv(1024).decode("utf-8")  # 接收客户端发送到服务端的消息，这里也会收到阻塞
    while msg:     # 允许接收客户端发送多次消息，如果对方发送空字符，则认为客户端断开连接，此时结束该线程
        print("客户端 %s 发送信息：%s" % (currentThread().name, msg))
        msg = client.recv(1024).decode("utf-8")

    print("客户端 %s 断开连接" % currentThread().name)

while True:
    print("等待接收客户端连接")
    client,addr = server.accept()    # 接受连接, 这里会受到阻塞

    # 创建线程用于客户端和服务端通信
    thread = Thread(target=contact, args=(client,))
    thread.start()







这是客户端代码
# coding=utf-8

from threading import Thread, currentThread
import socket

# 客户端代码

# 创建套接字
client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# 绑定ip和端口
ip = "127.0.0.1"
port = 8000

client.connect((ip, port))

while True:
    msg = input()
    if msg:
        client.send(msg.encode("utf-8"))
    else:   # 如果直接输入换行则断开连接
        client.close()
        break


分析如下：
服务端代码使用死循环接受多个客户端的连接，每有一个连接连进服务器就开一个线程用于客户端和服务端进行通信（其实是简单的输出客户端发送的消息而已）。

主线程只负责接收连接，不负责接受消息；其他线程则只负责接受消息与客户端通信。

如果不用多线程而是用单线程，把接收连接和接受消息都放在一个线程中
while True:
    print("等待接收客户端连接")
    client,addr = server.accept()	# 会阻塞
	
	msg = client.recv()		# 会阻塞
	
就会出现这样的问题：
由于accept 和 recv 都会阻塞，所以当有一个用户连接进来，但是它不发送消息的话，服务端就会被recv阻塞住。假如此时有其他客户端连接进来的话，也会被阻塞住，因为服务端的代码根本执行不到accept方法，服务端被recv阻塞住，所以其他客户端根本连不进来。

而开多线程可以解决这个问题。

上面执行 socket(),bind(),listen(),accept(),recv() 都会进行系统调用。也就是说，执行到这些方法的时候，都会进行用户态切换到内核态，把cpu让给内核程序。

像调用accept(),recv(),connect()方法进行IO操作会发生阻塞的情况就是BIO。BIO的模式需要每建立一个连接就占用一个线程用来通信。

所以BIO会出现以下问题：
1.当连接数很多的时候，创建的线程数也会很多，而线程间的切换会消耗cpu资源，也会损耗切换的时间。资源和时间就都浪费再线程的切换上了。cpu跑内核的系统调用就已经挺消耗挺大了，还要切换那么多线程，那不就浪费资源吗。
2.如果客户端不给服务端发送消息，但是又不断开连接，那么这个线程就相当于什么事都没有做，开一个线程却不做事情，这就是对线程资源的一种浪费。
3.断开连接时，线程就直接被销毁了，线程本来就是一种比较珍贵的资源，随便销毁也是对资源的浪费。

但是BIO的关键弊端其实不是线程太多，归根到底是因为blocking，是因为他是阻塞的，recv是阻塞的所以才需要开这么多线程，blocking是根本原因，线程太多只是结果。

那怎么办？

我们程序员无法做出改变，但是需要内核发生改变，因为recv是阻塞的这个事情是内核决定的。

很幸运，之后新版的内核提供了nonblock的socket，这就产生了NIO（同步非阻塞I/O模式）



NIO模型：同步非阻塞I/O模式
这种模式下，accept和recv，接收连接和接收消息都是非阻塞的。这么一来，我们就无需开多个线程用于通信，只需在一个线程中轮询多个客户端查看他们有没有发消息即可。

代码实现如下：
# coding=utf-8

from threading import Thread, currentThread
import socket
from time import sleep

# 服务端代码

# 创建套接字
server = socket.socket()

# 绑定ip和端口
ip = "127.0.0.1"
port = 8000
server.bind((ip, port))

server.listen(3)                # 只允许最多3个客户端连接
server.setblocking(False)       # 非阻塞socket
print("server服务已开启")

clients = dict()    # 用于存储所有建立了连接的客户端
no = 1          # 客户端编号
while True:
    try:
        client,addr = server.accept()    # 接收连接，非阻塞, 会马上返回。如果没有接收到连接则抛出一个异常
        print("接收到客户端")
        clients[no] = client
        no += 1
        client.setblocking(False)       # 设置客户端socket为非阻塞，这样后面调用recv就是非阻塞的了
    except BlockingIOError:
        # print("未接收到客户端")
        sleep(0.1)


    for client_no in list(clients.keys()):     # 遍历所有连接的客户端，接收他们发送的消息,这里要用list函数转一下clients.keys()，否则在删除字典中的客户端时再循环会报错说字典遇到改变
        each_client = clients[client_no]
        try:
            msg = each_client.recv(1024)      # 非阻塞,会马上返回。如果没有接收到消息则抛出一个异常

            if not msg:  # 如果发送空消息表示连接已断开
                each_client.close()
                del clients[client_no]  # 从列表中移除该客户端
                print("客户端 %s 断开连接" % str(client_no))
            else:
                print("客户端 %s 发送消息：%s" % (str(client_no), msg.decode('utf-8')))
                sleep(0.1)
        except BlockingIOError:     # 客户端未发送消息
            pass


NIO模型的优点是非阻塞所以避免了多线程，减少了线程间切换的开销

缺点：
要不断循环所有客户端查看客户端是否有发送消息。而且每一次循环都要调用accept和recv，也就是说，每一次循环都要进行系统调用，而系统调用本身就是一个比较消耗cpu的操作，涉及到用户态内核态的切换以及上下文切换。
也就是说通过循环的方式会很消耗cpu。

而且，不一定有客户端发送消息，假如有1万个客户端，只有一个客户端发送了消息，那么1万次轮询只有1次是有效的，因此用轮询的方式是很浪费，很没有效率的。

有没有什么办法，可以不通过轮询，而是让内核通知用户进程哪些客户端已经发送了消息可以去直接recv读取这些客户端的消息呢。


为了解决这个问题就提出了多路复用器。


多路复用器：

IO多路复用：通过一种机制（多路复用器），可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知用户进程进行相应的读写操作。

Linux内核提供的多路复用器有 select(), poll() 和 epoll()

多路复用器只会通知用户进程哪些文件描述符已经事件就绪和事件的类型，但不会自动将内核缓冲区的数据拷贝到用户进程的内存。用户进程需要自己调recv才能获取内核缓存区的客户端消息。因此多路复用还是一种同步IO模式。

如果是监控到客户端状态的同时还把客户端数据返回给用户程序，无需用户程序进行系统调用，那就是异步IO模型。

目前Linux内核支持的都是同步IO模型，而只有window的iocp支持这种异步io模型。





多路复用器之select()
可以在Linux中执行man 2 select 命令或者上网搜man 2 select查一下内核的select()方法

int select(int nfds, fd_set *readfds, fd_set *writefds,fd_set *exceptfds, struct timeval *timeout);
				  
nfds:传入文件描述符
readfds:文件描述符的读事件
writefds:文件描述符的写事件
exceptfds:文件描述符的异常事件

下面是select方法的描述
select() allows a program to monitor multiple file descriptors,waiting until one or more of the file descriptors become "ready" for some class of I/O operation
	   
它允许程序去监控多个文件描述符，直到一个或多个文件描述符变为“就绪”状态以待进行IO操作。

什么是文件描述符？简单的理解就是：Linux 中一切皆文件，比如 C++ 源文件、视频文件、Shell脚本、可执行文件等，就连键盘、显示器、鼠标等硬件设备也都是文件。一个 Linux 进程可以打开成百上千个文件，为了表示和区分已经打开的文件，Linux 会给每个文件分配一个编号（一个 ID），这个编号就是一个整数，被称为文件描述符（File Descriptor）。

这样说还远没有理解到文件描述符的本质，只是方便理解才这样说。

在本文的例子中，客户端和服务端socket对象就是文件描述符（socket也是文件描述符的一种）。select监视的就是客户端和服务端的socket对象。

下面文件描述符（file description）用fd简称表示。

接下来用select多路复用器实现一下上面的客户端和服务端模型：

# coding=utf-8

import socket
import select
# from select import select

# 服务端代码

# 创建套接字
server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# 绑定ip和端口
ip = "127.0.0.1"
port = 8000
server.bind((ip, port))

server.listen(3)                # 只允许最多3个客户端连接
server.setblocking(False)       # 非阻塞socket

rfd = set()     # 存放要监控的文件描述符，这里监控的是达到可读状态的socket
wfd = set()     # 存放要监控的文件描述符，这里监控的是达到可写状态的socket
efd = set()     # 存放要监控的文件描述符，这里监控的是出现异常状态的socket
rfd.add(server)

client_no = 1
clients = dict()

while True:
    print("开始select监控")
    # 获取所有处于可读，可写状态和发生异常的socket（他这里返回的到底是事件还是socket还不好说，可以验证一下）
    r_sockets, w_sockets, e_sockets = select.select(rfd, wfd, efd)      # 还可以传第四参超时时间，如果不传第四参，则select方法是阻塞的，执行到select就会一直阻塞直到有socket的状态改变；如果设了第4参为1，则执行select会阻塞1秒，1秒内没有socket改变状态就会循环1次再执行select

    # 这里我只关注可读的socket
    for r_socket in r_sockets:
        if r_socket == server:   # 如果是服务器socket可读，说明有连接进来了，此时可以去直接接收连接
            client, addr = r_socket.accept()      # 不阻塞，而且肯定可以接收到连接
            rfd.add(client)     # 将连接的客户端添加到要监控的集合中

            clients[client] = client_no
            print("客户端 %s 连接成功" % str(client_no))

            client_no += 1

        else:   # 如果是客户端socket可读，说明客户端发送消息到服务端
            msg = r_socket.recv(1024)       # 不阻塞

            if not msg:
                print("客户端 %s 断开连接" % str(clients[r_socket]))
                rfd.remove(r_socket)
            else:
                print("客户端 %s 发送消息：%s" % (str(clients[r_socket]), msg.decode('utf-8')))
				

讲解：
r_sockets, w_sockets, e_sockets = select.select(rfd, wfd, efd) 这句的意思是监视要监视的socket对象他们的状态。
以监视可读状态的socket为例，如果rfd中的某些socket状态变为可读，则将这些可读的socket返回给 r_sockets变量。
例如： rfd 集合中有 1~5 号，5个socket，2号和3号的状态变为可写，此时会将2号和3号这两个套接字传给r_sockets这个变量，以便对他们做出操作。

wfd和efd同理。


上面的代码中，首先让多路复用器select监视服务端socket（只监视它是否可读，不监视它是否可写或异常），由于此时socket是不可读的，所以select会阻塞程序，此时程序会让出cpu进入等待队列。一旦socket的状态变为可读（即有客户端连接进来），此时select会唤醒程序，并返回服务端socket给 r_sockets。

然后我得主动accept接收一下客户端的连接，并将刚连接进来的客户端soket也添加到要监视的可读状态的文件描述符集合rfd中。也就是说，我现在监视的就是 服务端socket和客户端1号的socket。

之后，如果有新的客户端连进来，都会被添加到被监视的集合中让select进行监控。

如果没有新客户端连接进来（服务端socket不可读，未达到读就绪状态），客户端也没有发送消息给服务端（客户端socket也没达到可读状态），select就会阻塞，直到有socket达到可写状态才会唤醒程序。
当有某些客户端发送消息进来的时候，select就监控到这几个客户端可读，然后将这些客户端socket返回到 r_sockets集合中，服务端就会对这几个客户端进行recv()。


总结：多路复用也是非阻塞IO
相比于普通的NIO模型，select模式好处在于不用每一次循环都所有已连接的客户端执行系统调用recv，而是可以监控发了消息的客户端，只对这些客户端执行系统调用。在有多个连接的情况下可以极大减少系统调用的次数，提高了效率又省性能。

poll其实和select的使用方式一样。
注意，每一次执行select的时候也是一次系统调用。


select的缺点：
1.每次while循环调用select(rfds,wfds,efds)都要将所有要监控的描述符（重复的）从用户空间拷贝一份到内核空间。
2.单个进程中，select监控的文件描述符有数量限制，不能超过1024个。
3.内核是通过遍历内核空间中所有的fd来监控fd的事件状态，会很消耗CPU。

为了解决上面的问题，推出了epoll模型。



多路复用器之epoll
相比与select和poll而言，epoll没有描述符限制，而且无需多次的将fd从用户空间拷贝到内核空间。

epoll提供了3个系统调用接口：

epoll_create(size):该函数在内核空间开辟一块新的空间（用来存放用户空间拷贝到内核空间的要监视的文件描述符），该函数返回一个epoll的文件描述符。size参数表示这个空间可以存多少个文件描述符。

epoll_ctl(int epfd,int op,int fd, struct epoll_event): 该函数是epoll的事件注册函数。这个函数作用是指定监控或者不再监控某个文件描述符fd的某一个事件（这里说的事件是指 accept,recv,send这些指令）。

第一参：epfd是epoll_create返回的epoll文件描述符，指代在内核空间中开辟的epoll空间

第二参：op是具体的操作，包括：EPOLL_CTL_ADD 注册新的fd到epfd（把fd拷贝到epoll空间）；EPOLL_CTL_DEL 从epfd删除一个fd；EPOLL_CTL_MOD 修改已注册的fd的监听事件。

第三参：fd, 需要监听的文件描述符，一般指socket_fd

第四参：event， 告诉内核要监听或者不监听该fd的哪个事件。


epoll_wait(epfd, epoll_event, maxevents, timeout)：等待注册事件的发生。timeout决定这个方法是阻塞的还是非阻塞的。0表示非阻塞，大于0表示阻塞。比如：timeout设为1，调用epoll_wait时内核会监控所有fd是否触发了注册的事件，阻塞1秒，1秒内还是没有fd触发注册的事件，就会返回-1；如果timeout设为0，则马上返回-1；如果timeout为-1，则会一直阻塞，直到有fd触发了事件为止。有fd触发了被监听的事件就叫做事件就绪。


epoll的常用事件如下：
EPOLLIN:读就绪  常量值为 1
EPOLLOUT：写就绪   4
EPOLLERR：服务端出现异常   8
EPOLLHUP: 客户端读写关闭（可以理解为client关闭了连接，但是连接还没有断开） 16

epoll有两种工作模式：LT（水平触发，默认） 和 ET（边缘模式）

LT：事件就绪后，用户可以选择处理或不处理，如果用户本次不处理，则下次调用epoll_wait仍会将未处理的事件打包给你

ET：事件就绪后，用必须处理，因为内核将就绪事件打包给你的时候就把对应的就绪事件从就绪列表中删除。

ET模式很大程度减少了fd从内核到用户空间的拷贝（减少了epoll_wait被触发的次数），效率比LT高。

==================================================================
epoll的工作具体流程和使用的数据结构如下：
1.当执行epoll_create()时，会在内核开辟一块内存空间用于存放要监控的fd，这块空间会包含两种数据结构：红黑树和双向链表（就绪链表），其中红黑树用于保存所有的要监控的fd，双向链表保存事件就绪的fd。
2.当执行epoll_ctl()时，会把要监控的fd从用户空间拷贝到内核空间，并为这个fd的对应事件注册一个回调函数，然后再将这个fd作为一个节点添加到红黑树中。
3.当一个fd的读事件或者写事件就绪时，会自动触发这个回调函数，回调函数要做的就是将这个fd添加到就绪链表中。
4.当调用 epoll_wait()时，会检测就绪链表的节点数是否为空，如果不为空说明有fd的事件已经就绪，此时就会将就绪链表中的fd拷贝回用户空间。让用户程序进行相应的读写操作。如果为空，epoll_wait()就会阻塞用户程序，直到有事件就绪。


epoll为什么使用红黑树存fd？
如果用户空间传入一个重复的fd，那么可以通过红黑树的高效查找（O(logn)的复杂度）找到这个fd节点已存在，就不会再把这个fd添加到树中。还有当某一个fd事件就绪的时候，内核也要找到这个fd将它放到就绪链表中。
还有删除一个fd，修改一个fd的事件类型（从监控读事件变为监控写事件），这些操作都必须要找到我要删或者要修改的那个fd，那么使用红黑树这个结构找起来就会非常的快。（树的分支存的是子节点fd的内存地址）
==================================================================


代码实例：
# coding=utf-8

import select, socket

# 服务端代码，该代码不能再windows中运行，因为windows中没有epoll的系统调用

server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
server.bind(("0.0.0.0", 8088))
server.listen()
server.setblocking(False)

print("服务器启动成功")

# 创建epoll对象，相当于执行了 epoll_create 在内核中开辟了一块空间用于记录fd事件
epoll = select.epoll()

# 注册要监听的fd事件，相当于调用 epoll_ctl 将fd的事件写入内核空间中。
epoll.register(server.fileno(), select.EPOLLIN)     # 监听server套接字的可读事件，第一参传的是服务端socket的文件描述符，文件描述符是一个整型数字

# 存储要监听的socket
monitored_socket = { server.fileno():server}

# 存储客户端发送过来的消息
client_msg = {}

timeout = 10

while True:
    # 等待fd的事件就绪（触发事件），相当于执行epoll_wait()，这个过程是阻塞的，上面的setblocking(False)是控制套接字的操作不阻塞，但是不能控制epoll不阻塞
    # 这里可以传入阻塞的超时时间timeout, 不传则一直阻塞; 如果没有事件就绪且超过超时时间则返回null;如果有事件就绪，则返回一个列表，每个列表放着一个元祖，元组放着事件就绪的fd和具体事件类型
    events = epoll.poll(timeout)

    if not events:
        print("无事件就绪")
    else:
        print("有 %s 个事件就绪" % len(events))

    print(events)

    for fd, event in events:
        ready_socket = monitored_socket[fd]     # 根据fd获取就绪的socket对象
        print("event", event)

        if ready_socket == server:      # 如果就绪fd是server，说明server可读事件就绪，表示有客户端连接
            client, addr = server.accept()     # 非阻塞，而且一定能接收到连接

            client_fd = client.fileno()

            print("客户端 %s 建立连接成功" % str(client_fd))

            client.setblocking(False)

            # 先监听客户端的可读事件
            epoll.register(client_fd, select.EPOLLIN)
            monitored_socket[client_fd] = client
            client_msg[client_fd] = []      # 保存该客户端的所有发送过来的消息

        elif event & select.EPOLLHUP:      # 如果客户端关闭连接

            ready_socket.close()

            epoll.unregister(fd)    # 不再监听该客户端的事件

            del monitored_socket[fd]

            del client_msg[fd]

            print("客户端 %s 关闭连接" % str(fd))

        elif event & select.EPOLLIN:  # 如果客户端读事件就绪
            msg = ready_socket.recv(1024)
            
            if msg:
                print("接收到客户端 %s 的消息 %s" % (str(fd), msg.decode("utf-8")))

                client_msg[fd].append(msg)

                # 修改监听事件为写事件,因为客户端发送消息过来之后我想将消息马上原样发送回给客户端
                epoll.modify(fd, select.EPOLLOUT)
            else:   # 如果返回空字符说明客户端关闭socket断开连接
                ready_socket.close()

                epoll.unregister(fd)  # 不再监听该客户端的事件

                del monitored_socket[fd]

                del client_msg[fd]

                print("客户端 %s 关闭连接" % str(fd))

        elif event & select.EPOLLOUT: # 如果客户端写事件就绪，其实只要客户端的输入缓冲区没满应该就满足写事件就绪（所谓的客户端写事件就绪是客户端可以被写而不是可以去写）
            try:
                msg = client_msg[fd].pop()
            except:
                print("客户端 %s 消息列表为空" % str(fd))
                epoll.modify(fd, select.EPOLLIN)  # 如果将客户端的所有消息都发回去了，就改回监听客户端读事件
            else:
                ready_socket.send(msg)




客户端代码：

# coding=utf-8

from threading import Thread
import socket

# 客户端代码

# 创建套接字
client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# 绑定ip和端口
ip = "127.0.0.1"
port = 8088

client.connect((ip, port))

def getResponse():
    while True:     # 开一个线程用于接收服务器返回的消息
        try:
            response = client.recv(1024)		# 阻塞
            print(response.decode("utf-8"))
        except:     # 如果主线程关闭客户端socket则recv会报错，此时break跳出循环，结束该线程即可
            print("子线程由于client断开连接而退出")
            break

thread_for_response = Thread(target=getResponse)
thread_for_response.start()

while True:
    msg = input()
    if msg:
        client.send(msg.encode("utf-8"))
        print("msg:" + msg)
    else:   # 如果直接输入换行则断开连接
        client.close()      # 关闭套接字断开连接
        print("client close")
        # client.shutdown(socket.SHUT_RDWR)
        break

PS：client.shutdown()不会断开连接，只是把读写关闭了而已，如果调用client.shutdown()，在服务端的表现就是客户端发送一个空字符给服务端，会触发客户端socket的EPOLLIN。所以我在EPOLLIN里面也加了一段关闭client并且从epoll对象中删除客户端fd的逻辑。

这个代码我把服务端代码放到我的远程服务器运行，客户端放到我的本机运行，不过要将客户端代码的连接ip改为我的远程服务器真实ip才行。

epoll，poll和select都是同步IO，因为他们需要在socket的事件就绪后由用户程序进行读写(recv，send)。

按照作者的说法：
在连接并发数高，连接活跃度不高（连接了之后不怎么发消息）的情况下，epoll比select更合适
在连接并发数不高，连接活跃度高的情况下，select比epoll更合适

对比select和epoll，epoll比较好的解决了select的3个缺点：
1.select有fd的数量限制默认是1024，epoll没有限制。
2.select通过在内核中遍历所有fd做到事件监控，epoll则通过注册回调函数的方式，当事件就绪时自动将fd放入就绪链表，检测fd的事件是否就绪只需查看就绪链表的节点是否为空即可。
3.在用户程序进行事件循环时，每次循环调用select()都要所有的fd重复的拷贝到内核空间，而epoll会将每次从用户空间拷贝到内核空间的fd保存到红黑树，所以每个fd只需拷贝1次无需重复拷贝，减少cpu的消耗。

https://www.cnblogs.com/longjiang-uestc/p/9605283.html
https://docs.python.org/3/library/selectors.html


========================================
补充知识：

什么是内核态和用户态
内核态和用户态本质上是不同的执行权级

Linux的执行权级：Linux提供了 0~3 共4个执行权级。
0是内核态，是最高权级，内核态状态下的进程可以做到访问任意地址的内存，操控硬件设备，控制CPU让出给哪个进程工作，这些内核态都能做到。

3是用户态，是最低权级，用户态状态下的进程只能访问本进程的内存，不能操作硬件设备。




什么是用户空间和内核空间
系统分配给每个进程一个独立的、连续的、虚拟的地址内存空间，该大小一般是4G（是所有进程都放在这里面共用）。其中将高地址值的内存空间分配给内核使用，一般是1G，其他空间给用户程序使用（即所有进程共用这3G）。
linux下每个进程的栈有两个，一个是用户态栈（用户空间），一个是内核态栈（内核空间）。
可以理解为用户空间和内核空间是存储在不同的内存空间中。




关于用户态切换内核态的过程：
Linux创建进程的时候，会给该进程分配两块空间：用户空间（用户栈）和内核空间（内核栈）。PCB进程控制块中保存着该进程的用户栈空间的地址和内核空间的地址。

CPU的寄存器中存储着当前用户程序的运行信息和上下文以及用户栈的地址。当cpu从用户态切换到内核态的时候（比如因为硬中断，硬件设备向CPU发起IRQ），首先会将用户程序的运行信息和上下文存到PCB的进程描述符（有点类似与游戏存档），然后cpu寄存器记录的堆栈地址从用户堆栈的地址指向为内核堆栈的地址（这就是用户空间切换到内核空间），CPU查询中断向量表在内核中找到对应的中断处理程序，并加载到CPU的寄存器，然后执行中断处理程序。
此时可以说，CPU被内核进程给占用。

上面就是用户态切换为内核态的过程。

内核态切换为用户态就是一个反过来的过程。


什么情况下会进行用户态切换内核态：
1.系统调用（用户程序自己发起中断，软中断）
2.外部设备发起中断请求（硬中断）
3.用户程序异常



用户态切换内核态与进程间切换的区别
CPU在两个进程间的切换本质上是CPU在两块PCB内存间的切换，CPU会从读取某块PCB切换为读取另一块PCB的数据，然后进行运算。
而用户态切换到内核态也是CPU从用户空间这块内存切换到内核空间这块内存。
所以二者都是CPU在不同内存间的切换。二者都需要进行用户程序的中断和上下文的保存。所以二者的耗时和成本相当。





系统中断的过程和分类

系统中断分为两种：硬中断和软中断
硬中断是由计算机硬件发起的中断，如网卡，鼠标，键盘和打印机等。硬中断可能发生在任何时期。
以网卡为例：当网卡接收到一个网络报文，报文由网卡的DMA（直接存储器访问）写入到内存（网卡缓冲区），网卡再向CPU发起一个中断请求（IRQ，interrupt request)，CPU收到中断信号会停下当前用户进程的运行，做好上下文环境的保存（保存到PCB的进程描述符中）。之后CPU从用户态切换到内核态（CPU所保存的堆栈地址从用户空间切换到内核空间的堆栈地址），执行网卡的中断程序。之后会切换回进程的用户态，CPU从进程描述符中读取上下文继续工作。

软中断是正在运行的用户态进程产生的，最常见的软中断就是用户程序要进行IO操作的时候，此时用户进程的上下文环境会从CPU的寄存器写入到内存中（写入到进程的进程描述符中，不是写入到用户空间中）以保存上下文。之后CPU由用户态切换到内核态进行系统调用，之后CPU会切换会刚才的用户态，加载上下文环境到CPU的寄存器中，然后继续用户进程的运行。除了io操作外，像sleep，yield代码也会产生软中断使当前进程让出CPU。

无论是硬中断还是软中断，每种系统中断都由各自不同的中断处理程序（即中断之后要执行的函数，要做的事情），例如系统调用他的中断处理程序就是 0x80 。像网卡，鼠标，键盘，硬盘都有它对应的中断处理程序。
这些中断处理程序的编号会以数字的形式存在CPU的中断向量表中。而中断处理程序的内容存在内核中，CPU会拿着这个编号去找内核中对应的中断处理程序来执行。

所以无论是硬中断还是软中断的io操作都需要进行用户态切换到内核态，因为要执行中断处理程序。






socket读写缓冲区工作机制

无论客户端socket还是服务端socket，都会有两个缓冲区：输入缓冲区（读缓冲区）和输出缓冲区（写缓冲区）

并不是说两个socket间建立连接后，A发送的数据可以直接到达B端。用户程序调用send时，用户程序的数据从用户空间拷贝一份到内核空间的输出缓冲区，send方法会立刻返回给用户空间，但是数据没有马上发送还留在缓冲区中，操作系统的TCP/IP协议程序会将缓冲区的数据封装为报文发送给对端。
对端发送消息过来时，数据先到内核空间的输入缓冲区，而不是直接就到了用户空间的程序中，用户程序调用read或者recv时，系统调用会将输入缓冲区的内容读到用户空间。

缓冲区是在内核空间的。recv和send会涉及到数据在用户空间和内核空间的拷贝和传输。

当socket.close()时，输出缓冲区的内容依旧可以发送到对端，但是输入缓冲区的内容就读不到用户空间了。


对于BIO模式的写，如果缓冲区空余大小小于要发送的数据大小，例如写入大小10mb,缓冲区总大小5mb，则会先写入5mb到输出缓冲区，等5mb的数据发送出去后，再将剩余5mb写入缓冲区，这个过程会有阻塞。
如果写入大小为2mb,缓冲区大小5mb,但是只有1mb的空余大小，则会等缓冲区留够2mb再写入到缓冲区，这个过程也会阻塞。

对于BIO模式的读，先检测输入缓冲区是否有数据，没有则阻塞，直到有数据。如果recv中传了参数指定读取长度，而且长度小于输入缓冲区中数据的长度，则要执行多次recv分次读取。


对于NIO模式的写，如果写入大小为2mb,缓冲区大小5mb,但是只有1mb的空余大小，则先写1mb到缓冲区，并返回给用户程序写入了多大的内容。如果缓冲区完全没空间，则直接返回-1，用户程序可以根据情况选择时阻塞还是重试，直到有空间写入。

对于NIO的读，有内容就读，没内容就返回空。


=====================================
socket套接字原理

socket编程是为了解决不同机器上两个进程之间的通信问题的。

我们创建套接字的时候总是需要往socket()中传入ip和端口。这是因为ip地址代表机器的位置，端口代表机器上的哪个进程（因为如果这个进程需要和其他进程通信就需要去绑定一个端口，所以端口代表一个进程）。

机器A的进程M如果和找到机器B的进程N通信就需要根据ip找到机器B，根据端口号找到进程N
（作者说，端口概念的引入是为了把一个ip掰开给多个进程使用）

总结： ip对应机器，port对应进程（应用程序）


=====================================
